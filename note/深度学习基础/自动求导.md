#### 向量链式法则

* ![image-20210715101311901](自动求导.assets/image-20210715101311901.png)
* 拓展到向量  形状最重要
  * ![image-20210715101324584](自动求导.assets/image-20210715101324584.png)
  * ![image-20210715101343454](自动求导.assets/image-20210715101343454.png)
  * ![image-20210715101355999](自动求导.assets/image-20210715101355999.png)

---

#### 自动求导

* 自动求导计算一个函数在指定值上的函数
* 有别于
  * 符号求导  给函数，求导  显式求导
    * ![image-20210715101409561](自动求导.assets/image-20210715101409561.png)
  * 数值求导   不知道f(x) 通过数值拟合导数
    * ![image-20210715101421811](自动求导.assets/image-20210715101421811.png)
* 计算图
  * 将代码分解成操作子    类似样例，分解步骤
  * 将计算表示成一个无环图
    * ![image-20210715101432769](自动求导.assets/image-20210715101432769.png)
  * 显示构造
    * Tensorflow/Theano/MXNet
    * ![image-20210715101445932](自动求导.assets/image-20210715101445932.png)
    * ![image-20210715101511671](自动求导.assets/image-20210715101511671.png)
      * 也可以隐式构造，告诉它，要隐藏构造，然后将所有的计算记录下来
  * 隐示构造
    * PyTorch/MXNet
* 自动求导的两种模式
  * 链式法则![image-20210715101524587](自动求导.assets/image-20210715101524587.png)
  * 正向积累![image-20210715101535613](自动求导.assets/image-20210715101535613.png)
  * 反向累计（反向传递）![image-20210715101552055](自动求导.assets/image-20210715101552055.png)
* 反向传递
  * ![image-20210715101618413](自动求导.assets/image-20210715101618413.png)
  * 步骤
    * 构造计算图
    * 前向：执行图，存储中间结果
    * 反向：从相反方向执行图
      * 去除不需要的枝
      * ![image-20210715101630832](自动求导.assets/image-20210715101630832.png)
  * 复杂度
    * 计算复杂度：O(n)，n是操作子个数
      * 通常正向和反向的代价类似
    * 内存复杂度：O(n)，需要存储正向的所有中间结果
    * 跟正向累计对比
      * O（n）计算复杂度用来计算一个变量的梯度
        * 每次都需要扫一遍操作树
      * O（n）内存复杂度

